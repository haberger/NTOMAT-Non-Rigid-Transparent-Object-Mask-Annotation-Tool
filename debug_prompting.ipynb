{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import time\n",
    "import cv2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "from utils.annotationdataset import AnnotationDataset\n",
    "from utils.annotationimage import AnnotationImage, AnnotationObject\n",
    "from utils.annotationscene import AnnotationScene\n",
    "from utils.voxelgrid import VoxelGrid\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "from copy import deepcopy   \n",
    "from scipy.ndimage import generic_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/envs/NTOMAT/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "sam_checkpoint = \"model_checkpoints/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = AnnotationScene(None, None, None, None, None)\n",
    "scene.scene_from_pickle(\"debug_data_promptgeneration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_voxels_in_scene(self, scene):\n",
    "    print(\"Starting voxel identification...\")\n",
    "\n",
    "    # Initialize camera intrinsics\n",
    "    intrinsics_start = time.time()\n",
    "    intrinsics = o3d.camera.PinholeCameraIntrinsic(\n",
    "        scene.img_width,\n",
    "        scene.img_height,\n",
    "        scene.camera_intrinsics[0, 0],\n",
    "        scene.camera_intrinsics[1, 1],\n",
    "        scene.camera_intrinsics[0, 2],\n",
    "        scene.camera_intrinsics[1, 2]\n",
    "    )\n",
    "    # Initialize visualizer\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "\n",
    "    num_ids = max(scene.scene_object_ids)+2\n",
    "\n",
    "    # Set of voxels\n",
    "    voxel_correspondences_global = {tuple(voxel.grid_index): np.zeros(num_ids, dtype=int) for voxel in self.o3d_grid.get_voxels()}\n",
    "\n",
    "    # Process each annotation image\n",
    "    for img_idx, image in enumerate(scene.annotation_images.values()):\n",
    "        if not image.annotation_accepted:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Get camera pose\n",
    "        pose = image.camera_pose\n",
    "\n",
    "        # Project voxel grid into image space\n",
    "        vis.create_window(width=scene.img_width, height=scene.img_height, visible=False)\n",
    "        vis.add_geometry(self.o3d_grid)\n",
    "        view_control = vis.get_view_control()\n",
    "        param = o3d.camera.PinholeCameraParameters()\n",
    "        param.intrinsic = intrinsics\n",
    "        param.extrinsic = np.linalg.inv(pose.tf)\n",
    "        view_control.convert_from_pinhole_camera_parameters(param, True)\n",
    "        vis.get_render_option().background_color = np.array([0, 0, 0])\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "        rgb = vis.capture_screen_float_buffer(True)\n",
    "        vis.destroy_window()\n",
    "\n",
    "        # Convert and scale grid positions\n",
    "        grid_position = np.array(rgb)\n",
    "        grid_position *= np.array([self.width/self.voxel_size, self.height/self.voxel_size, self.depth/self.voxel_size])\n",
    "        grid_position = np.round(grid_position).astype(np.int32)\n",
    "\n",
    "        # Remove background pixels and update voxel correspondences\n",
    "        valid_positions_mask = (grid_position != (0, 0, 0)).all(axis=-1)\n",
    "        valid_grid_positions = grid_position[valid_positions_mask]\n",
    "        valid_ids = image.get_complete_segmap()[valid_positions_mask]\n",
    "        for pos, voxel_id in zip(valid_grid_positions, valid_ids):\n",
    "            pos_tuple = tuple(pos)\n",
    "            if pos_tuple in voxel_correspondences_global:\n",
    "                voxel_correspondences_global[pos_tuple][voxel_id] += 1\n",
    "\n",
    "    # Deepcopy the grid for coloring\n",
    "    colored_voxel_grid = deepcopy(self.o3d_grid)\n",
    "\n",
    "    # Remove all voxels from the new grid\n",
    "    for voxel in colored_voxel_grid.get_voxels():\n",
    "        colored_voxel_grid.remove_voxel(voxel.grid_index)\n",
    "\n",
    "    # Filter and add colored voxels to the new grid\n",
    "    for key, value in voxel_correspondences_global.items():\n",
    "        if (value == 0).all():\n",
    "            continue\n",
    "        id = np.argmax(value)\n",
    "        if id != 0:\n",
    "            voxel = o3d.geometry.Voxel(key, [id / 255] * 3)\n",
    "            colored_voxel_grid.add_voxel(voxel)\n",
    "\n",
    "    # Filter out noise voxels\n",
    "    for voxel in colored_voxel_grid.get_voxels():\n",
    "        grid_index = tuple(voxel.grid_index)\n",
    "        count = sum(\n",
    "            np.argmax(voxel_correspondences_global.get((grid_index[0]+i, grid_index[1]+j, grid_index[2]+k), np.zeros(num_ids, dtype=int))) \n",
    "            == np.argmax(voxel_correspondences_global[grid_index])\n",
    "            for i in range(-1, 2)\n",
    "            for j in range(-1, 2)\n",
    "            for k in range(-1, 2)\n",
    "            if not (i == j == k == 0)\n",
    "        )\n",
    "        if count < 2:\n",
    "            colored_voxel_grid.remove_voxel(grid_index)\n",
    "    o3d.visualization.draw_geometries([colored_voxel_grid])\n",
    "    self.o3d_grid_id = colored_voxel_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting voxel identification...\n"
     ]
    }
   ],
   "source": [
    "identify_voxels_in_scene(scene.voxel_grid, scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_points_from_mask(mask, debug_visualization=True, just_one_point=False):\n",
    "\n",
    "    # skeleton = cv2.ximgproc.thinning(mask)\n",
    "\n",
    "    # for every connected component erode it until it has 10% of its original size\n",
    "    all_points = []\n",
    "    skeleton = np.zeros_like(mask)\n",
    "\n",
    "    if just_one_point:\n",
    "        segment_points = np.argwhere(mask > 0)\n",
    "        segment_points = segment_points.reshape(-1, 2)\n",
    "        initial_size = len(segment_points)\n",
    "        segment_mask = mask\n",
    "        while len(segment_points) > 0.1 * initial_size:\n",
    "            segment_mask = cv2.erode(segment_mask, np.ones((3, 3), np.uint8), iterations=1)\n",
    "            segment_points = np.argwhere(segment_mask > 0)\n",
    "            segment_points = segment_points.reshape(-1, 2)\n",
    "        skeleton += segment_mask\n",
    "        if len(segment_points) > 0:\n",
    "            criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "            _, _, centers = cv2.kmeans(segment_points.astype(np.float32), 1, None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
    "            all_points.extend(centers)\n",
    "    else:\n",
    "        num_labels, labels = cv2.connectedComponents(mask)\n",
    "        for label in range(1, num_labels):  # Start at 1 to skip the background label (0)\n",
    "            segment_mask = (labels == label).astype(np.uint8)\n",
    "            segment_points = np.argwhere(segment_mask > 0)\n",
    "            segment_points = segment_points.reshape(-1, 2)\n",
    "            initial_size = len(segment_points)\n",
    "            print(f\"Initial size: {initial_size}\")\n",
    "            if initial_size < 10:\n",
    "                continue\n",
    "\n",
    "            points = np.unique(labels, return_counts=True)[1]\n",
    "            num_points_per_segment = np.maximum(np.ceil(points / 3000), 1).astype(np.int32)\n",
    "\n",
    "            if len(segment_points) > 0:\n",
    "                while len(segment_points) > 0.1 * initial_size:\n",
    "                    segment_mask = cv2.erode(segment_mask, np.ones((3, 3), np.uint8), iterations=1)\n",
    "                    segment_points = np.argwhere(segment_mask > 0)\n",
    "                    segment_points = segment_points.reshape(-1, 2)\n",
    "                skeleton += segment_mask\n",
    "                if len(segment_points) > num_points_per_segment[label]:\n",
    "                    # Perform k-means\n",
    "                    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "                    _, _, centers = cv2.kmeans(segment_points.astype(np.float32), num_points_per_segment[label], None, criteria, 10, cv2.KMEANS_PP_CENTERS)\n",
    "\n",
    "                    # Snap centers to nearest skeleton points (corrected)\n",
    "                    distances = cdist(centers, segment_points)\n",
    "                    closest_point_indices = np.argmin(distances, axis=1)\n",
    "                    snapped_centers = segment_points[closest_point_indices]\n",
    "                    all_points.extend(snapped_centers)\n",
    "                elif len(segment_points) > 0:\n",
    "                    # If too few points for k-means, still include one\n",
    "                    center_index = np.random.choice(len(segment_points))\n",
    "                    all_points.append(segment_points[center_index])\n",
    "    centers = np.array(all_points, dtype=np.int32)\n",
    "        \n",
    "    if debug_visualization:\n",
    "        print(\"WTF\")\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(mask)\n",
    "        plt.imshow(skeleton, cmap='gray', alpha=0.5)\n",
    "        plt.scatter(centers[:, 1], centers[:, 0], c='r', s=10)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        #plt savefig\n",
    "        # plt.savefig('skeleton_img.png')\n",
    "\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_auto_prompts(image, scene, predictor):\n",
    "    '''\n",
    "    takes an annotation object and generates prompts for it\n",
    "    '''\n",
    "\n",
    "    #1. generate mask from voxelgrid\n",
    "    # -> get new segmentation map from voxelgrid (use offscreen rednerer project to pose)\n",
    "    voxelgrid = scene.voxel_grid\n",
    "    voxelgrid_segmap = voxelgrid.project_voxelgrid(scene.img_width, scene.img_height, scene.camera_intrinsics, image.camera_pose, voxelgrid.o3d_grid_id)\n",
    "    voxelgrid_segmap = voxelgrid_segmap[:,:,0]\n",
    "    #TODO add annotation_objects to scene_ids-> assign scene ids\n",
    "\n",
    "    #plt voxelgrid_segmap\n",
    "    #set figure size big \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(voxelgrid_segmap)\n",
    "    plt.show()\n",
    "\n",
    "    #2. for every object in annotation object\n",
    "\n",
    "    print(\"generating prompts\")\n",
    "\n",
    "    for obj in image.annotation_objects.values():\n",
    "        if obj.mask is not None:\n",
    "            print(\"mask not none\")\n",
    "            continue\n",
    "        mask = np.zeros_like(voxelgrid_segmap)\n",
    "        mask[voxelgrid_segmap == obj.scene_object_id] = 255\n",
    "        \n",
    "        #3. generate prompts\n",
    "        prompt_points = get_prompt_points_from_mask(mask, debug_visualization=True)\n",
    "\n",
    "        # visualize prompt point on rgb image\n",
    "        img = cv2.imread(image.rgb_path)\n",
    "        for point in prompt_points:\n",
    "            cv2.circle(img, (point[1], point[0]), 3, (0, 255, 0), -1)\n",
    "\n",
    "        for point in prompt_points:\n",
    "            image.active_object = obj\n",
    "            image.add_prompt([[point[1], point[0]]], [1], predictor)\n",
    "\n",
    "        for scene_object_id in scene.scene_object_ids:\n",
    "            if scene_object_id != obj.scene_object_id:\n",
    "                mask = np.zeros_like(voxelgrid_segmap)\n",
    "                mask[voxelgrid_segmap == scene_object_id] = 255\n",
    "                if np.sum(mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                prompt_points = get_prompt_points_from_mask(mask, debug_visualization=True, just_one_point=True)\n",
    "                for point in prompt_points:\n",
    "                    image.active_object = obj\n",
    "                    image.add_prompt([[point[1], point[0]]], [0], predictor)\n",
    "    print(\"prompts generated\")\n",
    "    # -> get mask from segmentation map\n",
    "    # -> generate positive prompts using get_prompt_points_from_mask\n",
    "    # -> generate negative prompts using get_prompt_points_from_mask for all other objects\n",
    "\n",
    "    # 3. for each prompt\n",
    "    # -> add prompt to annotation object using add_prompt\n",
    "\n",
    "    #TODO on write update obejcts.library\n",
    "    img = image.generate_visualization()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3267.558] global loadsave.cpp:241 findDecoder imread_('/home/daviddylan/interdisciplinary/Dataset/scenes/p_001/rgb/000072.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image\u001b[39m.\u001b[39mrgb_path, cv2\u001b[39m.\u001b[39mIMREAD_COLOR)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(rgb, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m predictor\u001b[39m.\u001b[39mset_image(rgb)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/david/work/NTOMAT-Non-Rigid-Transparent-Object-Mask-Annotation-Tool/debug_prompting.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m generate_auto_prompts(image, scene, predictor)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for image in scene.annotation_images.values():\n",
    "    if image.annotation_accepted:\n",
    "        continue\n",
    "    rgb = cv2.imread(image.rgb_path, cv2.IMREAD_COLOR)\n",
    "    rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "    predictor.set_image(rgb)\n",
    "    generate_auto_prompts(image, scene, predictor)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTOMAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
